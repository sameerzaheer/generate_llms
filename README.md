# llms.txt Generator

A deplloyable web app for automatically generating and maintaining `llms.txt` files for websites. This tool crawls websites, generates structured `llms.txt` content, and can optionally enhance the output using OpenAI's GPT models. Last functionality requires a valid OpenAI API key.

## Features

- **Web Crawling**: Automatically crawls websites to discover pages and content
- **llms.txt Generation**: Creates structured `llms.txt` files with proper formatting
- **LLM Enhancement**: Optional AI-powered content improvement using OpenAI GPT
- **Scheduled Updates**: Automatically monitors websites for changes and updates `llms.txt`
- **Compare to Firecrawl**: Compares the output to firecrawl
- **Configurable Crawling**: Customize crawling depth, excluded URLs, and update frequency

## Prerequisites

- Python 3.8 or higher
- OpenAI API key (for LLM functionality)
- Internet connection for web crawling

## Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd profound
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up OpenAI API key**:
   ```bash
   export OPENAI_API_KEY="your-openai-api-key-here"
   ```
   
   Or create a `.env` file in the project root:
   ```
   OPENAI_API_KEY=your-openai-api-key-here
   ```

## Usage

### Starting the Application

1. **Run the Flask application**:
   ```bash
   python run.py
   ```

2. **Access the web interface**:
   Open your browser and navigate to `http://localhost:5000`

### Using the Web Interface

#### Generator Tab
- **URL Input**: Enter the website URL you want to generate `llms.txt` for
- **Schedule Updates**: Check to create a recurring task that monitors the website
- **Use LLM**: Enable AI-powered content enhancement
- **LLM Instructions**: Provide custom instructions for content improvement
- **URL Substrings to Avoid**: List URL patterns to exclude from crawling
- **Trigger Interval**: Set how often scheduled tasks should run (in seconds)
- **Max Pages**: Limit the number of pages to crawl

#### Scheduled Tasks Tab
- View all scheduled tasks
- Monitor task execution status
- Copy generated `llms.txt` content
- Delete tasks when no longer needed

### Output Sections

The application generates three types of `llms.txt` content:

1. **Generated llms.txt**: Standard crawling output
2. **Generated llms.txt (with LLM)**: AI-enhanced version (if LLM is enabled)
3. **llms.txt generated by firecrawl**: Alternative crawling method

## Configuration

### Environment Variables

- `OPENAI_API_KEY`: Your OpenAI API key (required for LLM functionality)

### Crawling Settings

- **Max Pages**: Default 20 pages (configurable per task)
- **Max Depth**: Default 5 levels deep
- **Trigger Interval**: Default 70 seconds (configurable per task)
- **Avoid Substrings**: Custom URL patterns to exclude

## API Endpoints

- `GET /`: Main web interface
- `POST /generate`: Generate `llms.txt` for a URL
- `GET /scheduled-tasks`: List all scheduled tasks
- `POST /delete/<task_id>`: Delete a specific task

## Project Structure

```
profound/
├── run.py                 # Main Flask application
├── app/
│   ├── crawler.py         # Web crawling and llms.txt generation
│   └── alternatives.py    # Alternative crawling methods
├── templates/
│   └── index.html         # Web interface
├── requirements.txt       # Python dependencies
└── README.md             # This file
```

## Dependencies

### Core Dependencies
- Flask: Web framework
- APScheduler: Task scheduling
- requests: HTTP requests
- BeautifulSoup4: HTML parsing
- tldextract: Domain extraction

### LLM Dependencies
- openai: OpenAI API client

### Development Dependencies
- See `requirements.txt` for complete list

## Troubleshooting

### Common Issues

1. **Tasks disappearing**: Check console logs for task execution messages
2. **LLM not working**: Verify OpenAI API key is set correctly
3. **Crawling errors**: Check network connectivity and URL accessibility
4. **Scheduler issues**: Ensure APScheduler is running properly

### Debug Information

The application provides detailed logging:
- Task creation and deletion events
- Crawling progress and errors
- LLM processing status
- Scheduler job management

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

[Add your license information here]

## Support

For issues and questions:
- Create an issue on GitHub
- Check the troubleshooting section
- Review console logs for error messages 